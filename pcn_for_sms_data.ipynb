{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNert0kU4+dAiVytO75RmKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blacknahil/Predictive-coding-Network./blob/main/pcn_for_sms_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and preproccessing the csv file"
      ],
      "metadata": {
        "id": "b3gCFUx8kfBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "csv_file=\"sample_data/spam.csv\"\n",
        "\n",
        "\n",
        "data = pd.read_csv(csv_file, sep=\",\", encoding=\"latin-1\", usecols=[0, 1], names=[\"Label\", \"Message\"], skiprows=1)\n",
        "\n",
        "print(\"Loading the data done!\")\n",
        "print(\"the data head is \", data.head())\n",
        "\n",
        "\n",
        "data[\"Label\"] = data[\"Label\"].map({\"spam\":1,\"ham\":0})\n",
        "\n",
        "# vectorize\n",
        "\n",
        "vectorize = TfidfVectorizer(max_features=1000)\n",
        "X= vectorize.fit_transform(data[\"Message\"]).toarray()\n",
        "\n",
        "print(\"vectorizing the data done \")\n",
        "\n",
        "# Encode labels as one-hot vectors\n",
        "encoder= OneHotEncoder()\n",
        "\n",
        "Y = encoder.fit_transform(data[[\"Label\"]]).toarray()\n",
        "\n",
        "# split into training and validation sets\n",
        "\n",
        "X_train,X_valid,Y_train,Y_valid= train_test_split(X,Y,test_size=0.2,random_state=42)\n",
        "\n",
        "\n",
        "# save the data into .npy files\n",
        "\n",
        "np.save(\"sample_data/sms_spam/trainX.npy\",X_train)\n",
        "np.save(\"sample_data/sms_spam/trainY.npy\",Y_train)\n",
        "\n",
        "np.save(\"sample_data/sms_spam/validX.npy\",X_valid)\n",
        "np.save(\"sample_data/sms_spam/validY.npy\",Y_valid)\n",
        "\n",
        "\n",
        "print(\"Data processing Done!\")\n",
        "print(\"Files saved in sample_data/sms_spam folder \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6ldtmGzkefs",
        "outputId": "756832cb-c7ce-4e8a-eb78-46a3967aa96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the data done!\n",
            "the data head is    Label                                            Message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "vectorizing the data done \n",
            "Data processing Done!\n",
            "Files saved in sample_data/sms_spam folder \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing PCN and using it for sms data set using the preprocessed data"
      ],
      "metadata": {
        "id": "V5ueghuvoHKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE1NrRRwhKkf",
        "outputId": "7cfb3116-3616-4198-d28a-a67b51225f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-set: X: sample_data/sms_spam/trainX.npy | Y: sample_data/sms_spam/trainY.npy\n",
            "  Dev-set: X: sample_data/sms_spam/validX.npy | Y: sample_data/sms_spam/validY.npy\n",
            "sample_data/sms_spam/trainX.npy\n",
            "sample_data/sms_spam/trainY.npy\n",
            "--- Building Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING - Context doesn't have a save command registered. Saving all components\n",
            "WARNING:ngclogger:Context doesn't have a save command registered. Saving all components\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Simulation ---\n",
            "-1: Dev: Acc = 0.11479820311069489  NLL = 12.722171783447266 | Tr: Acc = 0.1191384345293045 EFE = --\n",
            "29 Dev: Acc = 0.8807175159454346, NLL = 0.072615846991539 | Tr: Acc = 0.8925285935401917, EFE = -0.5954628586769104\n",
            "------------------------------------\n",
            " Trial.sim_time = 0.15903820719983844 h  (572.5375459194183 sec)  Best Acc = 0.8852018713951111\n"
          ]
        }
      ],
      "source": [
        "from jax import numpy as jnp, random\n",
        "import sys, getopt as gopt, optparse, time\n",
        "from pcn import PCN ## bring in model from museum\n",
        "## bring in ngc-learn analysis tools\n",
        "from ngclearn.utils.metric_utils import measure_ACC, measure_CatNLL\n",
        "\n",
        "\n",
        "# read in general program arguments\n",
        "# external dataset arguments\n",
        "dataX = \"sample_data/sms_spam/trainX.npy\"\n",
        "dataY = \"sample_data/sms_spam/trainY.npy\"\n",
        "devX = \"sample_data/sms_spam/validX.npy\"\n",
        "devY = \"sample_data/sms_spam/validY.npy\"\n",
        "verbosity = 0 ## verbosity level (0 - fairly minimal, 1 - prints multiple lines on I/O)\n",
        "\n",
        "print(\"Train-set: X: {} | Y: {}\".format(dataX, dataY))\n",
        "print(\"  Dev-set: X: {} | Y: {}\".format(devX, devY))\n",
        "print(dataX)\n",
        "print(dataY)\n",
        "\n",
        "_X = jnp.load(dataX)\n",
        "_Y = jnp.load(dataY)\n",
        "\n",
        "Xdev = jnp.load(devX)\n",
        "Ydev = jnp.load(devY)\n",
        "x_dim = _X.shape[1]\n",
        "patch_shape = (int(jnp.sqrt(x_dim)), int(jnp.sqrt(x_dim)))\n",
        "y_dim = _Y.shape[1]\n",
        "\n",
        "n_iter = 30\n",
        "mb_size = 50\n",
        "n_batches = int(_X.shape[0]/mb_size)\n",
        "save_point = 20 ## save model params every modulo \"save_point\"\n",
        "\n",
        "## set up JAX seeding\n",
        "dkey = random.PRNGKey(1234)\n",
        "dkey, *subkeys = random.split(dkey, 10)\n",
        "\n",
        "## build model\n",
        "print(\"--- Building Model ---\")\n",
        "model = PCN(subkeys[1], x_dim, y_dim, hid1_dim=512, hid2_dim=512, T=20,\n",
        "            dt=1., tau_m=20., act_fx=\"sigmoid\", eta=0.001, exp_dir=\"exp\",\n",
        "            model_name=\"pcn\")\n",
        "model.save_to_disk() # save final state of synapses to disk\n",
        "print(\"--- Starting Simulation ---\")\n",
        "\n",
        "def eval_model(model, Xdev, Ydev, mb_size): ## evals model's test-time inference performance\n",
        "    n_batches = int(Xdev.shape[0]/mb_size)\n",
        "\n",
        "    n_samp_seen = 0\n",
        "    nll = 0. ## negative Categorical log liklihood\n",
        "    acc = 0. ## accuracy\n",
        "    for j in range(n_batches):\n",
        "        ## extract data block/batch\n",
        "        idx = j * mb_size\n",
        "        Xb = Xdev[idx: idx + mb_size,:]\n",
        "        Yb = Ydev[idx: idx + mb_size,:]\n",
        "        ## run model inference\n",
        "        yMu_0, yMu, _ = model.process(obs=Xb, lab=Yb, adapt_synapses=False)\n",
        "        ## record metric measurements\n",
        "        _nll = measure_CatNLL(yMu_0, Yb) * Xb.shape[0] ## un-normalize score\n",
        "        _acc = measure_ACC(yMu_0, Yb) * Yb.shape[0] ## un-normalize score\n",
        "        nll += _nll\n",
        "        acc += _acc\n",
        "\n",
        "        n_samp_seen += Yb.shape[0]\n",
        "\n",
        "    nll = nll/(Xdev.shape[0]) ## calc full dev-set nll\n",
        "    acc = acc/(Xdev.shape[0]) ## calc full dev-set acc\n",
        "    return nll, acc\n",
        "\n",
        "trAcc_set = []\n",
        "acc_set = []\n",
        "efe_set = []\n",
        "\n",
        "sim_start_time = time.time() ## start time profiling\n",
        "\n",
        "_, tr_acc = eval_model(model, _X, _Y, mb_size=1000)\n",
        "nll, acc = eval_model(model, Xdev, Ydev, mb_size=1000)\n",
        "print(\"-1: Dev: Acc = {}  NLL = {} | Tr: Acc = {} EFE = --\".format(acc, nll, tr_acc))\n",
        "if verbosity >= 2:\n",
        "    print(model._get_norm_string())\n",
        "trAcc_set.append(tr_acc) ## random guessing is where models typically start\n",
        "acc_set.append(acc)\n",
        "efe_set.append(-2000.)\n",
        "jnp.save(\"exp/dev_acc.npy\", jnp.asarray(acc_set))\n",
        "jnp.save(\"exp/efe.npy\", jnp.asarray(efe_set))\n",
        "\n",
        "for i in range(n_iter):\n",
        "    ## shuffle data (to ensure i.i.d. assumption holds)\n",
        "    dkey, *subkeys = random.split(dkey, 2)\n",
        "    ptrs = random.permutation(subkeys[0],_X.shape[0])\n",
        "    X = _X[ptrs,:]\n",
        "    Y = _Y[ptrs,:]\n",
        "\n",
        "    ## begin a single epoch\n",
        "    n_samp_seen = 0\n",
        "    train_EFE = 0. ## training free energy (online) estimate\n",
        "    trAcc = 0. ## training accuracy score\n",
        "    for j in range(n_batches):\n",
        "        dkey, *subkeys = random.split(dkey, 2)\n",
        "        ## sample mini-batch of patterns\n",
        "        idx = j * mb_size #j % 2 # 1\n",
        "        Xb = X[idx: idx + mb_size,:]\n",
        "        Yb = Y[idx: idx + mb_size,:]\n",
        "        ## perform a step of inference/learning\n",
        "        yMu_0, yMu, _EFE = model.process(obs=Xb, lab=Yb, adapt_synapses=True)\n",
        "        ## track online training EFE and accuracy\n",
        "        train_EFE += _EFE * mb_size\n",
        "        n_samp_seen += Yb.shape[0]\n",
        "        if verbosity >= 1:\n",
        "            print(\"\\r EFE = {} over {} samples \".format((train_EFE/n_samp_seen),\n",
        "                                                        n_samp_seen), end=\"\")\n",
        "    if verbosity >= 1:\n",
        "        print()\n",
        "\n",
        "    ## evaluate current progress of model on dev-set\n",
        "    nll, acc = eval_model(model, Xdev, Ydev, mb_size=1000)\n",
        "    _, tr_acc = eval_model(model, _X, _Y, mb_size=1000)\n",
        "    if (i+1) % save_point == 0 or i == (n_iter-1):\n",
        "        model.save_to_disk(params_only=True) # save final state of synapses to disk\n",
        "        jnp.save(\"exp/trAcc.npy\", jnp.asarray(trAcc_set))\n",
        "        jnp.save(\"exp/acc.npy\", jnp.asarray(acc_set))\n",
        "        jnp.save(\"exp/efe.npy\", jnp.asarray(efe_set))\n",
        "    ## record current generalization stats and print to I/O\n",
        "    trAcc_set.append(tr_acc)\n",
        "    acc_set.append(acc)\n",
        "    efe_set.append((train_EFE/n_samp_seen))\n",
        "    io_str = (\"{} Dev: Acc = {}, NLL = {} | \"\n",
        "              \"Tr: Acc = {}, EFE = {}\"\n",
        "             ).format(i, acc, nll, tr_acc, (train_EFE/n_samp_seen))\n",
        "    if verbosity >= 1:\n",
        "        print(io_str)\n",
        "    else:\n",
        "        print(\"\\r{}\".format(io_str), end=\"\")\n",
        "    if verbosity >= 2:\n",
        "        print(model._get_norm_string())\n",
        "if verbosity == 0:\n",
        "    print(\"\")\n",
        "\n",
        "## stop time profiling\n",
        "sim_end_time = time.time()\n",
        "sim_time = sim_end_time - sim_start_time\n",
        "sim_time_hr = (sim_time/3600.0) # convert time to hours\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "vAcc_best = jnp.amax(jnp.asarray(acc_set))\n",
        "print(\" Trial.sim_time = {} h  ({} sec)  Best Acc = {}\".format(sim_time_hr, sim_time, vAcc_best))\n",
        "\n",
        "jnp.save(\"exp/trAcc.npy\", jnp.asarray(trAcc_set))\n",
        "jnp.save(\"exp/acc.npy\", jnp.asarray(acc_set))\n",
        "jnp.save(\"exp/efe.npy\", jnp.asarray(efe_set))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F409YRPHkc4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "lHaNz2d-hxEu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wtPgY0roUUy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}